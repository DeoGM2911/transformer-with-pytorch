{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a74d79",
   "metadata": {},
   "source": [
    "# English-French Translation with Seq2Seq Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34afa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#  Add project directory\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0864737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.transformer import TransformerSeq2Seq, TransformerEncoder, TransformerDecoder\n",
    "from utils.loss import MaskedBCELoss\n",
    "from utils.train import minibatch_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c91733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.engfra_trans import MTEngFra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320d875",
   "metadata": {},
   "source": [
    "## Initialize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6baac690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "data_path = \"../data\"\n",
    "num_steps = 10\n",
    "num_train = 200\n",
    "num_val = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1142861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "dataset = MTEngFra(data_path, num_steps, num_train, num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501b3f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[47, 26,  1,  2,  3,  3,  3,  3,  3,  3],\n",
      "        [ 0, 50,  1,  2,  3,  3,  3,  3,  3,  3]], dtype=torch.int32), tensor([[ 0, 89,  2,  3,  4,  4,  4,  4,  4,  4],\n",
      "        [ 0, 89, 11,  3,  4,  4,  4,  4,  4,  4]], dtype=torch.int32), tensor([[15, 89, 18,  2,  3,  4,  4,  4,  4,  4],\n",
      "        [89, 11,  3,  4,  4,  4,  4,  4,  4,  4]], dtype=torch.int32), tensor([4, 4]))\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader\n",
    "for _, data in enumerate(dataset.data_loader(batch_size=2)):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680d404",
   "metadata": {},
   "source": [
    "## Prepare the model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94d11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the Transformer\n",
    "num_hiddens, enc_num_layers, dec_num_layers, dropout, batch_size = 32, 4, 8, 0.1, 32\n",
    "enc_num_heads, dec_num_heads, ffn_num_hiddens = 8, 16, 64\n",
    "src_vocab_size = len(dataset.src_vocab)\n",
    "tgt_vocab_size = len(dataset.tgt_vocab)\n",
    "\n",
    "# Hyperparameters for GD\n",
    "lr = 0.0001\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06452393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the modules\n",
    "encoder = TransformerEncoder(src_vocab_size, num_hiddens, enc_num_heads, enc_num_layers, ffn_num_hiddens, dropout)\n",
    "decoder = TransformerDecoder(tgt_vocab_size, num_hiddens, dec_num_heads, dec_num_layers, ffn_num_hiddens, dropout)\n",
    "model = TransformerSeq2Seq(encoder, decoder, dataset.tgt_vocab['<pad>'])\n",
    "\n",
    "# Criterions and Optimizer\n",
    "criterion = MaskedBCELoss(dataset.tgt_vocab['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f10a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the models\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3ef515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "====================================================================================================================================================================================\n",
       "TransformerSeq2Seq                                      [2, 5]                    [2, 5, 90]                --                        --                        --\n",
       "├─TransformerEncoder: 1-1                               [2, 5]                    [2, 5, 32]                --                        --                        --\n",
       "│    └─Embedding: 2-1                                   [2, 5]                    [2, 5, 32]                2,752                     --                        5,504\n",
       "│    └─PositionalEncoding: 2-2                          [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    └─Sequential: 2-3                                  --                        --                        --                        --                        --\n",
       "│    │    └─TransformerEncoderBlock: 3-1                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-1                [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-2                           [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-3                   [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-4                           [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerEncoderBlock: 3-2                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-5                [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-6                           [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-7                   [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-8                           [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerEncoderBlock: 3-3                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-9                [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-10                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-11                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-12                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerEncoderBlock: 3-4                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-13               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-14                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-15                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-16                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "├─TransformerDecoder: 1-2                               [2, 5]                    [2, 5, 90]                --                        --                        --\n",
       "│    └─Embedding: 2-4                                   [2, 5]                    [2, 5, 32]                2,880                     --                        5,760\n",
       "│    └─PositionalEncoding: 2-5                          [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    └─Sequential: 2-6                                  --                        --                        --                        --                        --\n",
       "│    │    └─TransformerDecoderBlock: 3-5                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-17               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-18                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-19               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-20                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-21                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-22                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-6                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-23               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-24                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-25               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-26                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-27                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-28                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-7                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-29               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-30                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-31               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-32                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-33                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-34                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-8                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-35               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-36                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-37               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-38                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-39                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-40                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-9                [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-41               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-42                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-43               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-44                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-45                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-46                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-10               [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-47               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-48                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-49               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-50                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-51                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-52                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-11               [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-53               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-54                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-55               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-56                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-57                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-58                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    └─TransformerDecoderBlock: 3-12               [2, 5, 32]                [2, 5, 32]                --                        --                        --\n",
       "│    │    │    └─MultiHeadAttention: 4-59               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-60                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─MultiHeadAttention: 4-61               [2, 5, 32]                [2, 5, 32]                4,128                     --                        8,256\n",
       "│    │    │    └─AddNorm: 4-62                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    │    │    └─PositionWiseFFN: 4-63                  [2, 5, 32]                [2, 5, 32]                4,192                     --                        8,384\n",
       "│    │    │    └─AddNorm: 4-64                          [2, 5, 32]                [2, 5, 32]                64                        --                        128\n",
       "│    └─Linear: 2-7                                      [2, 5, 32]                [2, 5, 90]                2,970                     --                        5,940\n",
       "====================================================================================================================================================================================\n",
       "Total params: 143,514\n",
       "Trainable params: 143,514\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.29\n",
       "====================================================================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 0.57\n",
       "Estimated Total Size (MB): 0.97\n",
       "===================================================================================================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample input to show the model structure\n",
    "dummy_batch_size = 2\n",
    "dummy_seq_length = 5\n",
    "dummy_x = torch.randint(0, src_vocab_size, (dummy_batch_size, dummy_seq_length))\n",
    "dummy_y = torch.randint(0, tgt_vocab_size, (dummy_batch_size, dummy_seq_length))\n",
    "dummy_z = torch.tensor([1, 2])\n",
    "\n",
    "# Display model summary\n",
    "summary(model, \n",
    "        input_data=[dummy_x, dummy_y, dummy_z],\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
    "        depth=4,\n",
    "        device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71477ecc",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0054b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "device = torch.device(\"cudo:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ab27fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study Materials\\PyTorch_ML_AI\\Attention and Transformer\\notebooks\\..\\model\\attention.py:108: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scores = torch.bmm(queries, keys.transpose(1, 2)) / torch.tensor(math.sqrt(d))  # (batch_size, num_queries, num_keys)\n",
      "d:\\Study Materials\\PyTorch_ML_AI\\Attention and Transformer\\notebooks\\..\\model\\attention.py:108: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scores = torch.bmm(queries, keys.transpose(1, 2)) / torch.tensor(math.sqrt(d))  # (batch_size, num_queries, num_keys)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m start = perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m minibatch_gd(model, dataset, batch_size, criterion, optimizer, device, num_epochs)\n\u001b[32m      3\u001b[39m end = perf_counter()\n\u001b[32m      4\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study Materials\\PyTorch_ML_AI\\Attention and Transformer\\notebooks\\..\\utils\\train.py:183\u001b[39m, in \u001b[36mminibatch_gd\u001b[39m\u001b[34m(model, dataset, batch_size, loss_fn, optimizer, device, num_epochs, log_dir)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m train_history = train_step(model, dataset, batch_size, loss_fn, optimizer, device, epoch, train_history)\n\u001b[32m    184\u001b[39m writer.add_scalar(\u001b[33m'\u001b[39m\u001b[33mLoss/train\u001b[39m\u001b[33m'\u001b[39m, train_history[epoch], epoch)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Log learning rate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study Materials\\PyTorch_ML_AI\\Attention and Transformer\\notebooks\\..\\utils\\train.py:56\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, dataset, batch_size, loss_fn, optimizer, device, epoch, train_history)\u001b[39m\n\u001b[32m     53\u001b[39m outputs = model(enc_X, dec_X, enc_valid_lens)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Compute loss (sum reduction to get batch total)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m loss = loss_fn(outputs.reshape(-\u001b[32m1\u001b[39m, outputs.shape[-\u001b[32m1\u001b[39m]), labels.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     57\u001b[39m batch_avg_loss = loss / batch_size\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\JDKs_Interpreters_Compliers\\Anaconda\\envs\\ml-env-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\JDKs_Interpreters_Compliers\\Anaconda\\envs\\ml-env-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Study Materials\\PyTorch_ML_AI\\Attention and Transformer\\notebooks\\..\\utils\\loss.py:40\u001b[39m, in \u001b[36mMaskedBCELoss.forward\u001b[39m\u001b[34m(self, pred, label)\u001b[39m\n\u001b[32m     37\u001b[39m mask = (label.reshape(-\u001b[32m1\u001b[39m) != \u001b[38;5;28mself\u001b[39m.pad).float()\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss(pred, label)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Apply the mask\u001b[39;00m\n\u001b[32m     43\u001b[39m loss = loss * mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\JDKs_Interpreters_Compliers\\Anaconda\\envs\\ml-env-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\JDKs_Interpreters_Compliers\\Anaconda\\envs\\ml-env-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\JDKs_Interpreters_Compliers\\Anaconda\\envs\\ml-env-cpu\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.cross_entropy(\n\u001b[32m   1294\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1295\u001b[39m         target,\n\u001b[32m   1296\u001b[39m         weight=\u001b[38;5;28mself\u001b[39m.weight,\n\u001b[32m   1297\u001b[39m         ignore_index=\u001b[38;5;28mself\u001b[39m.ignore_index,\n\u001b[32m   1298\u001b[39m         reduction=\u001b[38;5;28mself\u001b[39m.reduction,\n\u001b[32m   1299\u001b[39m         label_smoothing=\u001b[38;5;28mself\u001b[39m.label_smoothing,\n\u001b[32m   1300\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\JDKs_Interpreters_Compliers\\Anaconda\\envs\\ml-env-cpu\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3478\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.cross_entropy_loss(\n\u001b[32m   3480\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3481\u001b[39m     target,\n\u001b[32m   3482\u001b[39m     weight,\n\u001b[32m   3483\u001b[39m     _Reduction.get_enum(reduction),\n\u001b[32m   3484\u001b[39m     ignore_index,\n\u001b[32m   3485\u001b[39m     label_smoothing,\n\u001b[32m   3486\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "minibatch_gd(model, dataset, batch_size, criterion, optimizer, device, num_epochs)\n",
    "end = perf_counter()\n",
    "f'{end-start:.2f}s'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
